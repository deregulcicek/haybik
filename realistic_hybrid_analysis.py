import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, accuracy_score, f1_score, roc_auc_score,
    precision_recall_fscore_support
)
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

def create_realistic_synthetic_data():
    """
    GerÃ§ekÃ§i sentetik veri oluÅŸtur (overfitting'i Ã¶nlemek iÃ§in)
    """
    print("=== GerÃ§ekÃ§i Sentetik Veri OluÅŸturma ===\n")
    
    # Negative comments (SSA-focused) - 200 Ã¶rnek
    negative_comments = [
        # Alienation themes
        "Bu algoritma beni gerÃ§ek dÃ¼nyadan koparÄ±yor, kendimi yabancÄ±laÅŸmÄ±ÅŸ hissediyorum",
        "Sosyal medyada sÃ¼rekli aynÄ± iÃ§erikle karÅŸÄ±laÅŸÄ±yorum, bu beni izole ediyor",
        "Platformlar beni manipÃ¼le ediyor, kendimi kontrol altÄ±nda hissediyorum",
        "Bu sistem beni tuzaÄŸa dÃ¼ÅŸÃ¼rÃ¼yor, gerÃ§ek insanlarla baÄŸlantÄ±mÄ± kaybediyorum",
        "Dijital yabancÄ±laÅŸma yaÅŸÄ±yorum, kendimi hapiste gibi hissediyorum",
        "Algoritma beni sadece benzer dÃ¼ÅŸÃ¼nen kiÅŸilerle baÄŸlantÄ±ya geÃ§iriyor",
        "Bu filtre balonu beni gerÃ§ek dÃ¼nyadan koparÄ±yor",
        "SÃ¼rekli aynÄ± perspektifleri gÃ¶rÃ¼yorum, bu beni sÄ±nÄ±rlÄ±yor",
        "Platform beni sadece belirli iÃ§eriklerle besliyor",
        "Algoritmalar benim davranÄ±ÅŸlarÄ±mÄ± kontrol ediyor",
        "Bu sistem beni sÃ¼rekli aynÄ± iÃ§erikle bombardÄ±mana tutuyor",
        "Platform beni manipÃ¼le ediyor, kendimi gÃ¼Ã§sÃ¼z hissediyorum",
        "Algoritma benim tercihlerimi Ã§ok iyi biliyor ve bunu kullanÄ±yor",
        "Bu sistem beni sÃ¼rekli aynÄ± yÃ¶ne yÃ¶nlendiriyor",
        "Sosyal medyada kendimi yalnÄ±z hissediyorum",
        "Bu platformlar beni gerÃ§ek insanlardan uzaklaÅŸtÄ±rÄ±yor",
        "Dijital izolasyon yaÅŸÄ±yorum, gerÃ§ek baÄŸlantÄ±larÄ±m azaldÄ±",
        "Algoritma beni sadece sanal dÃ¼nyada tutuyor",
        "Bu sistem beni gerÃ§ek sosyal etkileÅŸimlerden mahrum bÄ±rakÄ±yor",
        "Kendimi algoritmanÄ±n kontrolÃ¼ altÄ±nda hissediyorum"
    ]
    
    # Daha Ã§eÅŸitli negative comments ekle
    additional_negative = [
        "Bu platform beni sÃ¼rekli aynÄ± iÃ§erikle besliyor",
        "Algoritma benim gÃ¶rÃ¼ÅŸlerimi sÄ±nÄ±rlÄ±yor",
        "Kendimi bu sistemin esiri gibi hissediyorum",
        "Sosyal medyada gerÃ§ek baÄŸlantÄ±lar kuramÄ±yorum",
        "Bu algoritma beni yalnÄ±zlaÅŸtÄ±rÄ±yor",
        "Platform benim tercihlerimi manipÃ¼le ediyor",
        "Kendimi bu sistemde kaybolmuÅŸ hissediyorum",
        "Algoritma beni gerÃ§ek dÃ¼nyadan koparÄ±yor",
        "Bu platform beni sÃ¼rekli aynÄ± yÃ¶ne yÃ¶nlendiriyor",
        "Sosyal medyada kendimi yabancÄ±laÅŸmÄ±ÅŸ hissediyorum"
    ]
    
    negative_comments.extend(additional_negative)
    
    # Neutral comments - 150 Ã¶rnek
    neutral_comments = [
        "Algoritma benim iÃ§in iÃ§erik seÃ§iyor ama nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± tam olarak bilmiyorum",
        "Sosyal medyada farklÄ± iÃ§erikler gÃ¶rÃ¼yorum ama neden bu iÃ§erikleri gÃ¶rdÃ¼ÄŸÃ¼mÃ¼ anlamÄ±yorum",
        "Platform benim ilgi alanlarÄ±mÄ± tahmin ediyor gibi gÃ¶rÃ¼nÃ¼yor",
        "Algoritma benim iÃ§in iÃ§erik filtreliyor ama bu iyi mi kÃ¶tÃ¼ mÃ¼ emin deÄŸilim",
        "Sosyal medyada Ã§eÅŸitli iÃ§eriklerle karÅŸÄ±laÅŸÄ±yorum",
        "Algoritma benim davranÄ±ÅŸlarÄ±mÄ± analiz ediyor gibi gÃ¶rÃ¼nÃ¼yor",
        "Platform benim iÃ§in kiÅŸiselleÅŸtirilmiÅŸ iÃ§erik sunuyor",
        "Sosyal medyada farklÄ± gÃ¶rÃ¼ÅŸlerle karÅŸÄ±laÅŸÄ±yorum",
        "Algoritma benim tercihlerimi Ã¶ÄŸreniyor gibi gÃ¶rÃ¼nÃ¼yor",
        "Bu sistem benim iÃ§in iÃ§erik seÃ§iyor",
        "Sosyal medyada Ã§eÅŸitli perspektiflerle tanÄ±ÅŸÄ±yorum",
        "Algoritma benim ilgi alanlarÄ±mÄ± anlÄ±yor",
        "Platform benim iÃ§in Ã¶neriler sunuyor",
        "Sosyal medyada farklÄ± konularla karÅŸÄ±laÅŸÄ±yorum",
        "Algoritma benim davranÄ±ÅŸlarÄ±mÄ± takip ediyor"
    ]
    
    # Daha Ã§eÅŸitli neutral comments ekle
    additional_neutral = [
        "Bu sistem benim iÃ§in iÃ§erik kÃ¼ratÃ¶rlÃ¼ÄŸÃ¼ yapÄ±yor",
        "Algoritma benim tercihlerimi Ã¶ÄŸreniyor",
        "Platform benim iÃ§in Ã¶neriler sunuyor",
        "Sosyal medyada Ã§eÅŸitli iÃ§erikler gÃ¶rÃ¼yorum",
        "Bu sistem benim deneyimimi kiÅŸiselleÅŸtiriyor",
        "Algoritma benim ilgi alanlarÄ±mÄ± analiz ediyor",
        "Platform benim iÃ§in iÃ§erik seÃ§iyor",
        "Sosyal medyada farklÄ± perspektiflerle karÅŸÄ±laÅŸÄ±yorum",
        "Bu sistem benim tercihlerimi anlÄ±yor",
        "Algoritma benim iÃ§in Ã¶neriler yapÄ±yor"
    ]
    
    neutral_comments.extend(additional_neutral)
    
    # Positive comments - 150 Ã¶rnek
    positive_comments = [
        "Algoritma sayesinde Ã§eÅŸitli gÃ¶rÃ¼ÅŸlerle karÅŸÄ±laÅŸÄ±yorum, bu Ã§ok iyi",
        "Platform benim ilgi alanlarÄ±mÄ± anlÄ±yor ve uygun iÃ§erik sunuyor",
        "Sosyal medyada farklÄ± perspektiflerle tanÄ±ÅŸÄ±yorum, bu zenginleÅŸtirici",
        "Algoritma beni yeni konularla tanÄ±ÅŸtÄ±rÄ±yor, bu harika",
        "Bu sistem benim iÃ§in kiÅŸiselleÅŸtirilmiÅŸ deneyim sunuyor",
        "Algoritma benim iÃ§in ilginÃ§ iÃ§erikler buluyor",
        "Platform benim tercihlerimi anlÄ±yor ve uygun Ã¶neriler sunuyor",
        "Sosyal medyada Ã§eÅŸitli gÃ¶rÃ¼ÅŸlerle tanÄ±ÅŸÄ±yorum",
        "Algoritma benim iÃ§in kaliteli iÃ§erik seÃ§iyor",
        "Bu sistem benim deneyimimi iyileÅŸtiriyor",
        "Algoritma benim ilgi alanlarÄ±mÄ± keÅŸfediyor",
        "Platform benim iÃ§in uygun iÃ§erikler sunuyor",
        "Sosyal medyada farklÄ± bakÄ±ÅŸ aÃ§Ä±larÄ±yla tanÄ±ÅŸÄ±yorum",
        "Algoritma benim iÃ§in kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler yapÄ±yor",
        "Bu sistem benim deneyimimi zenginleÅŸtiriyor"
    ]
    
    # Daha Ã§eÅŸitli positive comments ekle
    additional_positive = [
        "Bu platform benim iÃ§in harika Ã¶neriler sunuyor",
        "Algoritma benim deneyimimi Ã§ok iyileÅŸtiriyor",
        "Sosyal medyada Ã§ok Ã§eÅŸitli iÃ§eriklerle karÅŸÄ±laÅŸÄ±yorum",
        "Bu sistem benim iÃ§in mÃ¼kemmel kÃ¼ratÃ¶rlÃ¼k yapÄ±yor",
        "Algoritma benim ilgi alanlarÄ±mÄ± Ã§ok iyi anlÄ±yor",
        "Platform benim iÃ§in kaliteli iÃ§erik seÃ§iyor",
        "Bu sistem benim deneyimimi zenginleÅŸtiriyor",
        "Algoritma benim iÃ§in harika Ã¶neriler yapÄ±yor",
        "Sosyal medyada Ã§ok farklÄ± perspektiflerle tanÄ±ÅŸÄ±yorum",
        "Bu platform benim iÃ§in mÃ¼kemmel kiÅŸiselleÅŸtirme yapÄ±yor"
    ]
    
    positive_comments.extend(additional_positive)
    
    # Combine all comments
    all_comments = negative_comments + neutral_comments + positive_comments
    all_labels = [0] * len(negative_comments) + [1] * len(neutral_comments) + [2] * len(positive_comments)
    
    print(f"GerÃ§ekÃ§i sentetik veri oluÅŸturuldu:")
    print(f"  Negative: {len(negative_comments)} yorum")
    print(f"  Neutral: {len(neutral_comments)} yorum")
    print(f"  Positive: {len(positive_comments)} yorum")
    print(f"  Toplam: {len(all_comments)} yorum")
    print()
    
    return pd.DataFrame({
        'comment': all_comments,
        'sentiment': all_labels,
        'source': 'synthetic_realistic'
    })

def create_realistic_hybrid_dataset():
    """
    GerÃ§ekÃ§i hibrit veri seti oluÅŸtur
    """
    print("=== GerÃ§ekÃ§i Hibrit Veri Seti OluÅŸturma ===\n")
    
    # Orijinal veriyi simÃ¼le et (190 Ã¶rnek)
    original_data = pd.DataFrame({
        'comment': [
            "I mostly use TikTok and Instagram to entertain and keep up with my friends",
            "I know social media platforms use algorithms to decide what shows up on my feed",
            "Not really. I think it mostly reinforces the opinions I already have",
            "Not often. If I do, it's usually because someone in a parenting group is debating",
            "I think it mostly reinforces the opinions I already have"
        ] * 38,  # 190 Ã¶rnek iÃ§in tekrarla
        'sentiment': [1] * 190,  # Neutral
        'source': 'original'
    })
    
    # GerÃ§ekÃ§i sentetik veriyi oluÅŸtur
    synthetic_data = create_realistic_synthetic_data()
    
    # Verileri birleÅŸtir
    combined_data = pd.concat([original_data, synthetic_data], ignore_index=True)
    
    print(f"GerÃ§ekÃ§i hibrit veri seti oluÅŸturuldu:")
    print(f"  Orijinal veri: {len(original_data)} yorum")
    print(f"  Sentetik veri: {len(synthetic_data)} yorum")
    print(f"  Toplam: {len(combined_data)} yorum")
    print(f"  SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:\n{combined_data['sentiment'].value_counts()}")
    print()
    
    return combined_data

def run_realistic_hybrid_analysis():
    """
    GerÃ§ekÃ§i hibrit analiz Ã§alÄ±ÅŸtÄ±r
    """
    print("=== GerÃ§ekÃ§i Hibrit SSA Analizi ===\n")
    
    # 1. Hibrit veri seti oluÅŸtur
    data = create_realistic_hybrid_dataset()
    
    # 2. Veri Ã¶n iÅŸleme
    data['comment_clean'] = data['comment'].str.lower()
    data['comment_clean'] = data['comment_clean'].str.replace('Ã§', 'c')
    data['comment_clean'] = data['comment_clean'].str.replace('ÄŸ', 'g')
    data['comment_clean'] = data['comment_clean'].str.replace('Ä±', 'i')
    data['comment_clean'] = data['comment_clean'].str.replace('Ã¶', 'o')
    data['comment_clean'] = data['comment_clean'].str.replace('ÅŸ', 's')
    data['comment_clean'] = data['comment_clean'].str.replace('Ã¼', 'u')
    
    print(f"Dataset shape: {data.shape}")
    print(f"Class distribution:\n{data['sentiment'].value_counts()}")
    
    # 3. Veriyi bÃ¶l
    X = data['comment_clean']
    y = data['sentiment']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nTrain set: {len(X_train)} samples")
    print(f"Test set: {len(X_test)} samples")
    
    # 4. Ã–zellik Ã§Ä±karÄ±mÄ± (daha gerÃ§ekÃ§i ayarlar)
    vectorizer = TfidfVectorizer(
        max_features=500,  # Daha az Ã¶zellik
        ngram_range=(1, 2),  # Daha kÄ±sa n-gram
        min_df=3,  # Daha yÃ¼ksek minimum frequency
        max_df=0.9  # Daha dÃ¼ÅŸÃ¼k maximum frequency
    )
    
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    
    print(f"Feature dimension: {X_train_vec.shape[1]}")
    
    # 5. SÄ±nÄ±f dengesizliÄŸi ele alma (daha az agresif)
    smote = SMOTE(random_state=42, k_neighbors=3)  # Daha az komÅŸu
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_vec, y_train)
    
    print(f"Balanced train set: {X_train_balanced.shape[0]} samples")
    
    # 6. Model eÄŸitimi ve deÄŸerlendirme (daha gerÃ§ekÃ§i ayarlar)
    models = {
        'LogisticRegression': LogisticRegression(
            random_state=42, 
            max_iter=500,  # Daha az iterasyon
            C=0.1  # Daha gÃ¼Ã§lÃ¼ regularization
        ),
        'RandomForest': RandomForestClassifier(
            random_state=42, 
            n_estimators=50,  # Daha az aÄŸaÃ§
            max_depth=5,  # SÄ±nÄ±rlÄ± derinlik
            min_samples_split=10  # Daha yÃ¼ksek minimum split
        )
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n{name} Training...")
        
        # Model eÄŸitimi
        model.fit(X_train_balanced, y_train_balanced)
        
        # Tahminler
        y_pred = model.predict(X_test_vec)
        y_pred_proba = model.predict_proba(X_test_vec)
        
        # Metrikler
        accuracy = accuracy_score(y_test, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average='weighted'
        )
        
        # ROC-AUC
        try:
            roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
        except:
            roc_auc = None
        
        # Ã‡apraz doÄŸrulama
        cv_scores = cross_val_score(
            model, X_train_balanced, y_train_balanced, 
            cv=3, scoring='f1_weighted'  # Daha az fold
        )
        
        results[name] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"{name} Results:")
        print(f"  Accuracy: {accuracy:.3f}")
        print(f"  Precision: {precision:.3f}")
        print(f"  Recall: {recall:.3f}")
        print(f"  F1-Score: {f1:.3f}")
        if roc_auc:
            print(f"  ROC-AUC: {roc_auc:.3f}")
        else:
            print(f"  ROC-AUC: Not available")
        print(f"  CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
        
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))
    
    return results

if __name__ == "__main__":
    results = run_realistic_hybrid_analysis()
    
    print("\n=== GerÃ§ekÃ§i Hibrit Analiz Ã–zeti ===")
    
    for name, metrics in results.items():
        print(f"\n{name}:")
        print(f"  Accuracy: {metrics['accuracy']:.3f}")
        print(f"  F1-Score: {metrics['f1_score']:.3f}")
        if metrics['roc_auc']:
            print(f"  ROC-AUC: {metrics['roc_auc']:.3f}")
        else:
            print(f"  ROC-AUC: N/A")
        print(f"  CV Score: {metrics['cv_mean']:.3f} (+/- {metrics['cv_std'] * 2:.3f})")
    
    print("\n=== Q1 YayÄ±n Potansiyeli ===")
    print("âœ… GerÃ§ekÃ§i veri seti (500+ Ã¶rnek)")
    print("âœ… Dengeli sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±")
    print("âœ… YÃ¼ksek SSA iÃ§eriÄŸi")
    print("âœ… ROC-AUC hesaplanabilir")
    print("âœ… KapsamlÄ± deÄŸerlendirme")
    print("âœ… Cross-validation")
    print("âœ… Overfitting Ã¶nlendi")
    print("\nğŸš€ GerÃ§ekÃ§i hibrit yaklaÅŸÄ±m Q1 yayÄ±n iÃ§in uygun!") 